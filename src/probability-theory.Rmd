---
title: "Probability Theory"
output:
  html_document:
    keep_md: yes
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris
  pdf_document: default
---

```{r knitr_opts, cache=FALSE, include=FALSE}
knitr::opts_chunk$set(fig.path = "img/",
                      autodep = TRUE,
                      dpi = 300,
                      fig.asp = 0.5)
```

>How could we build a machine which would carry out useful plausible reasoning, following clearly defined principles expressing an idealized common sense?  

E.T. Jaynes' Probability Theory  

The ideal of systematizing the scientific process
has been a major pursuit of natural philosophers,
such as Aristotle, Francis Bacon, Laplace and the Logical Positivists,
and their descendants, whom we now call scientists.
Aristotelian logic in particular was concerned with
the truth of propositions such as
"Socrates is an animal".
The major tool of Aristotle's system was deduction,
drawing conclusions that necessary follow from an argument.
Much of the later work was focused on providing 
the theoretical foundations for induction,
defined by Aristotle as "argument from the particular to the universal" 
(https://plato.stanford.edu/entries/aristotle-logic/).
Arguably most of modern science is concerned with induction
--infering general conclusions from particular evidence.
The question in the opening quote can serve as a pragmatic abstraction of 
the primordial ideal in the setting of inductive inference.
Jaynes's thesis was that probability theory can be viewed as
our best answer in the attempt to provide a unifying system
that becomes a generalization of Aristotelian logic
extended to inferential reasoning.
Based on a reading of Jaynes' book^[1], "Probability Theory: The Logic of Science",
we will produce an exposition of theoretical and practical issues
in light of his arguments on the foundations of probability theory.

Explain a bit parts of the opening quote.
Why should that be our goal?
While we may be able to adequately evaluate the relative merit of 
two propositions or hypotheses that are clearly distinguished from each other,
we are helpless once the number of hypotheses or of the relevant parameters
increases even slightly beyond that.
We need a formalized system,
a mathematical theory,
to help us reason through those more complicated problems
and a machine to crunch the numbers.
Choosing a machine as the subject that is supposed to execute the reasoning
has the added benefit,
except from making it clear that the goal is to write a computer program,
of avoiding the subjective/objective debate regarding prior information: 
the machine would benefit to use all availabe information, 
no matter what we call it.
Turning the problem into an engineering challenge
makes explicit that the challenge is
to choose the appropriate design principles for our goal
rather than attain a potentially ill-defined truth,
as has been the case for Logical Positivists.
Jaynes defines plausible reasoning as a synonym to inference,
i.e. finding the most plausible answer to a question 
based on the available information under uncertainty.
Reference to "common sense" is not to say that 
human reasoning is infallible and always consistent, 
but rather that if our conclusions disagree with some very basic desiderata 
we should discard the conclusions and the process that lead us to them.

Basic desiderata:

I) Degrees of plausibility are represented by real numbers (what is plausibility)
II) Qualitative Correspondence with common sense
III) Consistent reasoning: a) If a conclusion can be reasoned out in more than one way, then every possible way must lead to the same result. b) The robot always takes into account all of the evidence it has relevant to a question. It does not arbitrarily ignore some of the information, basing its conclusions only on what remains. In other words, the robot is completely non-ideological. c) The robot always represents equivalent states of knowledge by equivalent plausibility assignments. That is, if in two problems the robot's state of knowledge is the same (except perhaps for the labelling of the propositions), then it must assign the same plausibilities in both.

Two rules follow based on those desiderata from Cox's functional equations:
1) The product rule
$$ p(AB|C) = p(A|BC)w(B|C) = p(B|AC)w(A|C) $$ (define symbols)
2) The sum rule
$$ p(A|B) + p(\overbar{A}|B) = 1 $$

>Aristotelian deductive logic is the limiting form of our rules for plausible reasoning, as the robot becomes more and more certain of its conclusions.

Issues to discuss:
* Subjective vs objective (> Anyone who has the same information but comes to a different conclusion than our robot, is necessarily violating one of those desiderata.)  
* Randomness is not a property of the real world (Mind projection fallacy): shaking an urn does not affect the natural properties of anything, it just makes it impossible that any human will be able to willfully influence the result. It is absurd to think that randomization makes our calculations exact (see CLT). 
> The notion of "physical probability" must retreat continually from one level to the next, as knowledge advances.  
* >In virtually all real problems of scientific inference we are in just the opposite situation: the data $D$ are known but the correct hypothesis $H$ is not. Then the problem facing the scientist is of the inverse type; given the data $D$, what is the probability that some specified hypothesis $H$ is true?
> "What do you know about the hypothesis $H$ after seeing the data $D$?" cannot have any defensible answer unless we take into account: "What did you know abouth $H$ before seeing $D$?" 
Optional stopping, regularization, "multiple imputation?" are taken care of by default with Bayesian inference
Inference on small samples is as exact as is for large samples, which is not true with frequentist approach
How to choose the prior: prior predictive plots for prior elicitation, the exact parameter values when uncertain usually do not matter, unless sample is small in which case you should explicitly mention your prior as just choosing uniform priors may also affect the results and is rarely defensible.
Need to realize that the frequentist approach is equivalent to the Bayesian approach when using uniform priors, which is fine and faster, as long as you are honest in that you do not have other information
The practical advantage that you learn the simple unifying principles enabling you to fit the model that you data deserve (bespoke) instead of having to learn seemingly unrelated, ad hoc methods with the frequentist approach (regularization, ancillary statistics, "multiple imputation?", robust methods, meta-analysis, multiple comparison correction). Note that this also means that with frequentist statistics the prior is often hidden in the choice of model and the procedure that one settles on one model over the other is not reported or is not principled. Now that the computers are getting powerful enough, this is evident by the flexibility of Bayesian analysis packages such as brms which can fit very elaborate models with incrementally more effort, instead of the leaps required by someone to learn every new frequentist technique (this needs polishing)

I won't go into Maximum entropy (see Gelman's post) although it seems very powerful in certain settings
Uncertainty is not a property of the world, it's epistemic all the way through.
Jaynes' theory is far-reaching, ambitious as it ties together logic and information theory (which I haven't talked about yet), yet is based on apparently defensible desiderata.

Link to article pondering whether Bayesian methods should be the default.
Based on what we have argued, it probably should be unless the model is simple, you have little to no prior information, and enough data are available, such as in industrial settings and simple crop experiments, like the problems on which Fisher was working on, unless of course you find the Bayesian interpretation of posterior probabilities useful in those case as well.

Jaynes' polemic tone is not widespread nowadays, and that's for the better.
As Bayesianism re-gains traction, it is not needed. 
Many mix-and-match approaches are widely used (multiple imputation in frequentist inference, regularization which can be seen as one way to incorporate priors).

Limitations: computational overhead (but perhaps variational inference can help in the future), limited adoption in some communities hinders communication


[1]: Mostly the first two chapters where he unfolds the foundations of Probability Theory as logic. The rest of the book just extends and further explains the corollaries of those first two chapters.

## References  

