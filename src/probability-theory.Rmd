---
title: "Probability Theory"
output:
  html_document:
    keep_md: yes
  md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris
  pdf_document: default
---

```{r knitr_opts, cache=FALSE, include=FALSE}
knitr::opts_chunk$set(fig.path = "img/",
                      autodep = TRUE,
                      dpi = 300,
                      fig.asp = 0.5)
```

>How could we build a machine which would carry out useful plausible reasoning, following clearly defined principles expressing an idealized common sense?  

E.T. Jaynes' Probability Theory  

The ideal of systematizing the scientific process
has been a major pursuit of natural philosophers,
such as Aristotle, Francis Bacon, Laplace, the Logical Positivists,
and their descendants, whom we now call scientists.
Aristotelian logic, in particular, was concerned with
the truth of propositions such as
"Socrates is an animal".
The major tool of Aristotle's system was deduction,
drawing conclusions that necessarily follow from an argument.
Much of the later work was focused on providing 
the theoretical foundations for induction,
defined by Aristotle as "argument from the particular to the universal" 
(https://plato.stanford.edu/entries/aristotle-logic/).
Arguably most of modern science is concerned with induction
---infering general conclusions from particular evidence.
The question in the opening quote can serve as a pragmatic abstraction of 
the primordial ideal in the setting of inductive inference.
Jaynes's thesis was that probability theory can be viewed as
our best answer in the attempt to provide a unifying system
that becomes a generalization of Aristotelian logic
extended to inferential reasoning.
Based on a reading of Jaynes' book^[1], "Probability Theory: The Logic of Science",
I will produce an exposition of theoretical and practical issues
in light of his arguments on the foundations of probability theory.

## Probability Theory as extended logic  

Why should that be our goal?
While we may be able to adequately evaluate the relative merit of 
two propositions or hypotheses that are clearly distinguished from each other,
we are helpless once the number of hypotheses or of the relevant parameters
increases even slightly beyond that.
We need a formalized system,
a mathematical theory,
to help us reason through those more complicated problems
--and a machine to crunch the numbers.
Choosing a machine as the subject that is supposed to execute the reasoning
makes it clear that the goal is to write a computer program
and has the added benefit of avoiding 
the subjective/objective debate regarding prior information: 
the machine would benefit to use all available information, 
no matter what we call it.
Turning the problem into an engineering challenge
makes explicit that the challenge is
to choose the appropriate design principles to achieve our goal
rather than attain a potentially ill-defined truth,
as has been the case for Logical Positivists.
Jaynes defines "plausible reasoning" as a synonym to inference,
i.e. finding the most plausible answer to a question 
based on the available information under uncertainty.
The usage of "common sense" does not imply that 
human reasoning is infallible and always consistent, 
but rather that if our conclusions disagree with some very basic desiderata 
we should discard the conclusions and the process that lead us to them.

Jaynes sets up a set of basic desiderata as a starting point for his reasoning:

I) Degrees of plausibility are represented by real numbers  

This is deemed necessary as the machine is required to 
associate plausibility assignments with some physical quantity
such as voltage or bits.
It does not appear as objectionable to adopt the convention that
a greater number corresponds to greater plausibility
and that an infinitesimal change corresponds to an
infinitesimal change in plausibility.

II) Qualitative correspondence with common sense

This basically means that the direction of change 
must be qualitatively consistent with our intuition.
For example, if our machine acquires new information $C'$ 
that updates the previously held information $C$
in a way that the plausibility of $A$ given $C'$ is increased
compared to the plausibility of $A$ given $C$

$$ A|C' > A|C  $$

but the plausibility of $B$ is unchanged

$$  B|AC' = B|AC $$

then we should expect that a) the joint plausibility of $A$ and $B$ 
can only increase and never decrease

$$  AB|C' \geq AB|C $$

and b) a decrease in the plausibility that $A$ is false, $\overline{A}$.

III) Consistent reasoning: 

a) If a conclusion can be reasoned out in more than one way, 
then every possible way must lead to the same result.  
b) The robot always takes into account all of 
the evidence it has relevant to a question. 
It does not arbitrarily ignore some of the information, 
basing its conclusions only on what remains. 
In other words, the robot is completely non-ideological.  
c) The robot always represents equivalent states of knowledge 
by equivalent plausibility assignments. 
That is, if in two problems the robot's state of knowledge is the same 
(except perhaps for the labelling of the propositions), 
then it must assign the same plausibilities in both.  

It is perhaps surprising at first that the above-stated conditions
uniquely identify one set of mathematical operations that obey them.

Two rules follow based on those desiderata from Cox's functional equations:

1) The product rule:

If $A$ and $B$ are two statements,
we seek to calculate the plausibility of their conjunction, $AB$,
given that $C$ is true, $p(AB|C)$.
If $B$ is true given $C$,
then $B|C$ is true.
For both $A$ and $B$ to be true given that $B|C$ is true
then $A|BC$ also needs to be true.
Putting everything together,
the plausibility of $AB|C$ must be equal the plausibility of both
$A|BC$ and $B|C$

$$ p(AB|C) = p(A|BC)p(B|C) $$
But since we could just as well swap the labels of the statements
$A$ and $B$,
based on the desideratum III the following relationship 
should also hold equivalently:

$$ p(AB|C) = p(B|AC)p(A|C) $$
The product rule combines our results:

$$ p(AB|C) = p(A|BC)p(B|C) = p(B|AC)p(A|C) $$ 

2) The sum rule
$$ p(A|B) + p(\overline{A}|B) = 1 $$

merely expresses the basic Aristotelian postulate that
if $A$ is true, then not-$A$, or $\overline{A}$, must be false, and reversely.

>Aristotelian deductive logic is the limiting form of our rules for plausible reasoning, as the robot becomes more and more certain of its conclusions.

## Discussion  

Accepting these seemingly unobjectionable corollaries 
leads to the adoption of 
[Bayesian inference](http://www.scholarpedia.org/article/Bayesian_statistics)

$$ p(H | E) = \frac{p(E | H) p(H)}{p(E)} $$
where $p(H|E)$ is called the posterior distribution,
the probability of the hypothesis $H$ given the evidence $E$,
$p(H)$ is the prior probability,
the probability of the hypothesis before observing the data,
and $p(E)$ is the normalizing factor when multiple hypotheses are considered,
for example when considering two competing hypotheses $H_1$ and $H_2$,
$p(E) = p(E|H_1)p(H_1) + p(E|H_2)p(H_2)$.
For those trained initially within the frequentist framework,
working under the Bayesian framework leads to questions
regarding the choice of priors 
and a re-evaluation of the concept of uncertainty.
Bayesian inference comes with some benefits in estimating relatively complex
models that are not as easy to fit within the frequentist framework and 
a simplification of their interpretation, as well as some drawbacks, 
mostly revolving around computational cost
We will briefly discuss those issues in the following sections.

### Priors  

Even for people who accept the conclusions 
of the logical analysis presented in the previous section,
the choice of the prior probability is seen as a thorny issue.
One objection raised is often that the prior can only be subjectively chosen,
something which is seen as contradicting the objectivity pursued 
in scientific practice.
However, whereas the choice of the prior probability is in one sense subjective,
as it is relative to the information available to the agent doing the reasoning,
it is also objective in the sense that 
any agent that has access to the same information 
must choose the same prior probability.
There remains a practical issue of translating our prior beliefs into 
specific values of prior probabilities.
An effective way to choose the priors 
is to inspect prior predictive distributions
which provide a way to examine the range and likely values
of responses our priors would produce
before seeing any data:

$$ p(y, \theta) = p(y|\theta)p(\theta) $$
where $p(\theta)$ is our prior distribution
and $p(y|\theta)$ is the sampling distribution.
If the prior predictive distribution generates values
that are beyond what we would consider reasonable
we must adjust our priors accordingly.
Of course this method of prior elicitation 
can only narrow down the range of parameter values 
---the exact parameter values are often impossible to define,
unless we have very specific information 
from previous studies or other expert knowledge.
In cases where the sample size is adequately large
or the prior is not so specific that its influence dominates that of the data,
the choice of values within that reasonable range
will make little difference in the end result.
If, however, the choice of the prior makes a difference for the results,
it is arguably a useful thing to report it 
so that people can evaluate its reasonableness.

The alternative of using frequentist inference,
which is equivalent to assumming a uniform prior^[2],
is rarely a defensible approach 
since there is always some additional information:
the coefficient of your experimental variable 
is *a priori* unlikely to take values equal to $- \infty$, 
the Avogadro constant, and everything in between with equal plausibility.
Using uniform priors results in other problems as well,
such as edge effects at the parameter space boundaries
and an inability to converge in the presence 
of collinearity among the predictors.
Granted, there are ways to circumvent those problems in some cases
under the frequentist framework.
For instance, the latter problem that can arise due to collinearity
can be ameliorated by regularization approaches.
But it is easy to show [REF: Elements of Statistical Learning] that these are in turn
equivalent to choosing appropriate priors under the Bayesian framework.
The difference is that the choice of the equivalent Bayesian priors
is more principled and can help avoid issues of "double dipping",
i.e. using the data twice, 
once to optimise the hyperparameters of the frequentist regularization 
and to then later produce inferences based on them,
something that undermines their predictive ability.

### Conceptual juxtaposition of Bayesian and frequentist inference  

The basic goal of the frequentist approach is to
estimate how likely are the data we observed, $y$,
given a set of fixed parameters, $\theta$,
designated as $p(y|\theta)$.
It thus considers that the uncertainty in the data is irreducible, or aleatoric.
Data distributions are chosen based on imaginary projections
to future samples generated from the same process.
The Bayesian approach is designed to estimate
how likely a hypothesis $H$ is,
given the data $D$ we observed, $p(H|D)$.
Hence, it takes the observed data as definite,
measurement errors and all,
and considers that the final source of uncertainty is epistemic,
arising due to lack of information.
Since often our hypotheses are defined
to encompass a range of values
probability distributions are asigned over a model's parameters.

The tension between the two approaches on this matter is a philosophical issue
that cannot be easily resolved.
One the one hand, randomness can always be reduced 
to progressively lower levels of description,
but on the other it is an open question whether it can be completely eliminated.
For example, the exact position an arrow is going to end its trajectory
can be difficult to determine
---that is why archery is an interesting sport to some.
But, in principle, if we had information about all the relevant parameters,
down to a complete description of the wind patterns and the gravitational field,
we should be able to precisely predict the arrow's final position.
Ultimately, it boils down to the question of whether
the universe is deterministic, which must involve 
some discussion around the state of quantum physics
that still appears to remain unresolved.
 
In practice, though, it rarely is relevant for experimenters
to invoke levels of description that involve quantum physics.
The choice of analytical framework is most often made
based on the particular goals of the scientific question at hand.
In situations where the models specified are relatively simple
and the samples considered are large enough
it is sufficient to adopt the frequentist approach
to get an accurate answer fast.
However, in many actual scientific problems
the underlying models are, or should be,
complicated to a degree that the frequentist approach does not provide
satisfactorily powerful tools
that are as easy to implement.
Furthermore, if the question asked by the experimenter is 
"What have I learned from the data regarding my hypothesis
after seeing the data?",
it cannot be reasonably answered unless 
the available information about the hypothesis before seeing the data 
is also specified.

PAEDAGOGICAL VALUE
Another important factor based on which someone may prefer the Bayesian approach
is that it more closely matches
the way that scientists tend to think
as is evident by the observation that it is very often the case
that frequentist $p$-values are misinterpreted
as Bayesian posterior probabilities
---as if answering the question 
"*How likely is my hypothesis given the observed data?*".

DISCUSS ALL AD-HOC TOOLS
Another important point is that within the standard frequentist framework
it is not possible to evaluate the plausibility that your model's
estimated parameter values are within or outside a range,
although *ad hoc* tools, such as 
[equivalence tests](https://doingbayesiandataanalysis.blogspot.com/2017/02/equivalence-testing-two-one-sided-test.html), 
have been proposed.

Mis-interpretation of p-values as if they were Bayesian posterior probabilities

question whether the universe is deterministic - quantum physics
that I am not prepared to attempt a full exposition.
However, in practice, the Bayesian approach matches better the way scientists
Often, though, what experimenters want to know is the reverse,
which can and often does involve 
a range of parameter values instead of single values,
think: if you care about ... then you must tell us what you knew ...
Of course, someone can choose to be "externally inconsistent" and act as if all uncertainty is aleatoric if it simplifies the calculations, which is a reasonable approach for simple models.

* Subjective vs objective (> Anyone who has the same information but comes to a different conclusion than our robot, is necessarily violating one of those desiderata.)  
How to choose the prior: prior predictive plots for prior elicitation, the exact parameter values when uncertain usually do not matter, unless sample is small in which case you should explicitly mention your prior as just choosing uniform priors may also affect the results and is rarely defensible.
Need to realize that the frequentist approach is equivalent to the Bayesian approach when using uniform priors, which is fine and faster, as long as you are honest in that you do not have other information
> "What do you know about the hypothesis $H$ after seeing the data $D$?" cannot have any defensible answer unless we take into account: "What did you know abouth $H$ before seeing $D$?" 

* >In virtually all real problems of scientific inference we are in just the opposite situation: the data $D$ are known but the correct hypothesis $H$ is not. Then the problem facing the scientist is of the inverse type; given the data $D$, what is the probability that some specified hypothesis $H$ is true?
* Randomness is not a property of the real world (Mind projection fallacy): shaking an urn does not affect the natural properties of anything, it just makes it impossible that any human will be able to willfully influence the result. It is absurd to think that randomization makes our calculations exact (see CLT). 
> The notion of "physical probability" must retreat continually from one level to the next, as knowledge advances.  
Uncertainty is not a property of the world, it's epistemic all the way through.

Optional stopping, regularization, "multiple imputation?" are taken care of by default with Bayesian inference
Inference on small samples is as exact as is for large samples, which is not true with frequentist approach
The practical advantage that you learn the simple unifying principles enabling you to fit the model that you data deserve (bespoke) instead of having to learn seemingly unrelated, ad hoc methods with the frequentist approach (regularization, ancillary statistics, "multiple imputation?", robust methods, meta-analysis, multiple comparison correction). Note that this also means that with frequentist statistics the prior is often hidden in the choice of model and the procedure that one settles on one model over the other is not reported or is not principled. Now that the computers are getting powerful enough, this is evident by the flexibility of Bayesian analysis packages such as brms which can fit very elaborate models with incrementally more effort, instead of the leaps required by someone to learn every new frequentist technique (this needs polishing)

I won't go into Maximum entropy (see Gelman's post) although it seems very powerful in certain settings
Jaynes' theory is far-reaching, ambitious as it ties together logic and information theory (which I haven't talked about yet), yet is based on apparently defensible desiderata.

Link to article pondering whether Bayesian methods should be the default.
Based on what we have argued, it probably should be unless the model is simple, you have little to no prior information, and enough data are available, such as in industrial settings and simple crop experiments, like the problems on which Fisher was working on, unless of course you find the Bayesian interpretation of posterior probabilities useful in those case as well.

Jaynes' polemic tone is not widespread nowadays, and that's for the better.
As Bayesianism re-gains traction, it is not needed. 
Many mix-and-match approaches are widely used (multiple imputation in frequentist inference, regularization which can be seen as one way to incorporate priors).

Limitations: computational overhead (but perhaps variational inference can help in the future), limited adoption in some communities hinders communication


[1]: Mostly the first two chapters where he unfolds the foundations of Probability Theory as logic. The rest of the book just extends and further explains the corollaries of those first two chapters.

[2]: Even if useful to understand some differences between the Bayesian and the frequentist frameworks, this statement is simplifying some of the underlying conceptual differences which we will discuss later.

## References  

